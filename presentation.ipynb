{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rag](data/images/rag.jpg \"RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI key from env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "azure_version = \"2024-06-01\"\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_reasoning = os.getenv(\"AZURE_OPENAI_REASONING\")\n",
    "azure_embeddings = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\")\n",
    "azure_whisper = os.getenv(\"AZURE_OPENAI_WHISPER\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_key = os.getenv(\"AZURE_OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llm instance\n",
    "\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=azure_key,\n",
    "    api_version=azure_version,\n",
    "    azure_deployment=azure_deployment,\n",
    "    model=azure_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Pub quizzes, also known as quiz nights or trivia nights, are social events typically held in pubs or bars. They have become a popular form of entertainment, particularly in the United Kingdom, Ireland, the United States, Australia, and other English-speaking countries. Here's a general overview of what you might expect at a pub quiz:\\n\\n### Format:\\n- **Teams:** Participants usually form teams, which can range from a couple of people to larger groups, depending on the venue's rules.\\n- **Rounds:** The quiz is divided into several rounds, each focusing on different types of questions or themes. Common categories might include general knowledge, sports, music, movies, history, science, or specific themed rounds.\\n- **Question Types:** Questions can be multiple-choice, true/false, picture rounds (identify images or people), audio rounds (identify songs or sound clips), and written answers.\\n\\n### Organization:\\n- **Quizmaster:** There is typically a host or quizmaster who reads out questions, tracks scores, and provides the correct answers.\\n- **Materials:** Teams are usually provided with answer sheets and sometimes pens. Some pubs might use digital systems for answering.\\n- **Timing:** The quiz might last anywhere from a couple of hours to a whole evening, with breaks between rounds for socializing, eating, or drinking.\\n\\n### Scoring and Prizes:\\n- **Scoring:** Points are awarded for each correct answer, and the team with the most points at the end of the final round is declared the winner.\\n- **Prizes:** Prizes for winning teams can vary widely and might include cash, bar tabs, drinks, food vouchers, or trivia-related items like books or games. Some quizzes also have jackpot rounds or rollover prizes that increase in value if not won.\\n\\n### Social and Cultural Aspects:\\n- **Community Building:** Pub quizzes are often seen as a way to bring people together, fostering a sense of community and camaraderie.\\n- **Regular Events:** Many pubs host regular quiz nights, often weekly or monthly, which can create a loyal following.\\n- **Charity Events:** Occasionally, pub quizzes are organized to raise funds for charitable causes, with entry fees or proceeds being donated.\\n\\n### Variations and Popularity:\\n- **Popularity:** The popularity of pub quizzes has led to the formation of quiz leagues and competitions at regional and national levels, with some events drawing large audiences.\\n- **Online Quizzes:** In recent years, especially during the COVID-19 pandemic, virtual pub quizzes have gained popularity, allowing people to participate remotely.\\n\\nOverall, pub quizzes are a fun and engaging way for people to test their knowledge, socialize, and enjoy a night out.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 533, 'prompt_tokens': 16, 'total_tokens': 549, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-21c70435-0415-41af-9f0d-897ec1d9676e-0', usage_metadata={'input_tokens': 16, 'output_tokens': 533, 'total_tokens': 549, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke llm\n",
    "\n",
    "llm.invoke(\"What do you know about Pub Quizzes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke llm\n",
    "\n",
    "answer = llm.invoke(\"What do you know about Pub Quizzes?\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pub quiz is a quiz held in a public house or bar. These events are commonly organized as a form of light entertainment, bringing together groups of patrons who form teams to compete by answering questions on various topics. Pub quizzes typically cover a wide range of subjects, including general knowledge, history, pop culture, music, sports, and more.\n",
      "\n",
      "The format of a pub quiz can vary, but it generally involves several rounds of questions, with each round focused on a specific topic or type of question. Some rounds might include picture questions, audio clips, or even interactive tasks. Participants write down their answers and submit them for scoring at the end of each round. The team with the highest score at the end of the quiz usually wins a prize, which can vary from free drinks to vouchers or even cash prizes.\n",
      "\n",
      "Pub quizzes have become a popular social activity in many countries, particularly in the United Kingdom, Ireland, and Australia, and have seen a rise in popularity in other parts of the world as well. They provide a fun way for people to test their knowledge, engage in friendly competition, and socialize with friends and other patrons.\n"
     ]
    }
   ],
   "source": [
    "# use prompts\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"You are an encyclopedia.\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke({\"input\": \"What are Pub Quizzes?\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusue prompts\n",
    "\n",
    "answer = chain.invoke({\"input\": \"What are Pub Quizzes also called?\"})\n",
    "print(answer.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream\n",
    "\n",
    "for token in chain.stream({\"input\": \"What are Pub Quizzes also called?\"}):\n",
    "    print(token.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called?\",\n",
    "        \"context\": \"A pub quiz is a quiz held in a pub or bar. These events are also called quiz nights,[1] trivia nights,[2] or bar trivia[3] and may be held in other settings. The pub quiz is a modern example of a pub game, and often attempts to lure customers to the establishment on quieter days. The pub quiz has become part of British culture since its popularization in the UK in the 1970s by Burns and Porter, although the first mentions in print can be traced to 1959.[4][5] It then became a staple in Irish pub culture, and its popularity has continued to spread internationally. Although different pub quizzes can cover a range of formats and topics, they have many features in common. Most quizzes have a limited number of team members, offer prizes for winning teams, and distinguish rounds by category or theme.\",\n",
    "    }\n",
    ")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"A pub quiz is a quiz held in a pub or bar. These events are also called quiz nights,[1] trivia nights,[2] or bar trivia[3] and may be held in other settings. The pub quiz is a modern example of a pub game, and often attempts to lure customers to the establishment on quieter days. The pub quiz has become part of British culture since its popularization in the UK in the 1970s by Burns and Porter, although the first mentions in print can be traced to 1959.[4][5] It then became a staple in Irish pub culture, and its popularity has continued to spread internationally. Although different pub quizzes can cover a range of formats and topics, they have many features in common. Most quizzes have a limited number of team members, offer prizes for winning teams, and distinguish rounds by category or theme.\",\n",
    "        metadata={\n",
    "            \"source\": \"wikipedia\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called and what is the source?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document prompts\n",
    "\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\"Content: {page_content}                             \n",
    "Source: {source}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called and what is the source?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "wikipedia.search(\"Pub Quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.page(\"Pub Quiz\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def retrieve_wikipedia_page(input):\n",
    "    search = wikipedia.search(input[\"input\"])\n",
    "    if not search:\n",
    "        return []\n",
    "    page = wikipedia.page(search[0])\n",
    "    return [Document(\n",
    "        page_content=page.content,\n",
    "        metadata={\n",
    "            \"source\": \"wikipedia\"\n",
    "        }\n",
    "    )]\n",
    "\n",
    "runnable = RunnableLambda(retrieve_wikipedia_page)\n",
    "runnable.invoke({\"input\": \"Pub Quiz\"})\n",
    "\n",
    "chain = create_retrieval_chain(\n",
    "    runnable, document_chain\n",
    ")\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called and what is the source?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_retrieval_chain(\n",
    "    runnable, document_chain\n",
    ")\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called and what is the source?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector stores\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# load data\n",
    "loader = TextLoader(r\"./data/text/pubquiz.txt\", encoding=\"utf-8\")\n",
    "data = loader.load()\n",
    "loader = TextLoader(r\"./data/text/llm.txt\", encoding=\"utf-8\")\n",
    "data.extend(loader.load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separators=[\".\", \"\\n\"])\n",
    "documents = splitter.split_documents(data)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=azure_key,\n",
    "    api_version=azure_version,\n",
    "    azure_deployment=azure_embeddings,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory=\"./chroma_1/presentation\")\n",
    "\n",
    "for document in documents:\n",
    "    print(document)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "db = Chroma(persist_directory=\"./chroma_1/presentation\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "retriever.invoke(\"Pub quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke({\"input\": \"LLM\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieval_chain.invoke({\"input\": \"What are Pub Quizzes also called? Cite the Source!\"})\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieval_chain.invoke({\"input\": \"Give a concise description of a LLM. Cite the Source! Write every sentence in a newline.\"})\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "Text: {context}\"\"\")\n",
    "\n",
    "summary_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "answer = summary_chain.invoke(\n",
    "    {\"context\": documents}\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_document\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableParallel, RunnablePassthrough\n\u001b[0;32m----> 9\u001b[0m partial_format_document \u001b[38;5;241m=\u001b[39m partial(format_document, prompt\u001b[38;5;241m=\u001b[39m\u001b[43mdocument_prompt\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# The chain we'll apply to each individual document.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Returns a summary of the document.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m map_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: partial_format_document}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m|\u001b[39m summary_prompt\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'document_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain.chains.combine_documents import collapse_docs, split_list_of_docs\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "\n",
    "# The chain we'll apply to each individual document.\n",
    "# Returns a summary of the document.\n",
    "map_chain = (\n",
    "    {\"context\": partial_format_document}\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# A wrapper chain to keep the original Document metadata\n",
    "map_as_doc_chain = (\n",
    "    RunnableParallel({\"doc\": RunnablePassthrough(), \"content\": map_chain})\n",
    "    | (lambda x: Document(page_content=x[\"content\"], metadata=x[\"doc\"].metadata))\n",
    ").with_config(run_name=\"Summarize (return doc)\")\n",
    "\n",
    "# The chain we'll repeatedly apply to collapse subsets of the documents\n",
    "# into a consolidate document until the total token size of our\n",
    "# documents is below some max size.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(partial_format_document(doc) for doc in docs)\n",
    "\n",
    "collapse_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Collapse this content:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def get_num_tokens(docs):\n",
    "    return llm.get_num_tokens(format_docs(docs))\n",
    "\n",
    "def collapse(\n",
    "    docs,\n",
    "    config,\n",
    "    token_max=400,\n",
    "):\n",
    "    collapse_ct = 1\n",
    "    while get_num_tokens(docs) > token_max:\n",
    "        config[\"run_name\"] = f\"Collapse {collapse_ct}\"\n",
    "        invoke = partial(collapse_chain.invoke, config=config)\n",
    "        split_docs = split_list_of_docs(docs, get_num_tokens, token_max)\n",
    "        docs = [collapse_docs(_docs, invoke) for _docs in split_docs]\n",
    "        collapse_ct += 1\n",
    "    return docs\n",
    "\n",
    "# The chain we'll use to combine our individual document summaries\n",
    "# (or summaries over subset of documents if we had to collapse the map results)\n",
    "# into a final summary.\n",
    "\n",
    "reduce_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Combine these summaries:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_config(run_name=\"Reduce\")\n",
    "\n",
    "# The final full chain\n",
    "map_reduce = (map_as_doc_chain.map() | collapse | reduce_chain).with_config(run_name=\"Map reduce\")\n",
    "\n",
    "answer = map_reduce.invoke(\n",
    "    input=documents[:5],\n",
    "    config={\"max_concurrency\": 5},\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um den Preis für 3 Kilogramm Kartoffeln zu berechnen, müssen wir zuerst den Preis pro Kilogramm ermitteln.\n",
      "\n",
      "Ein Sack mit 4 Kilogramm Kartoffeln kostet 8 Euro. Daher beträgt der Preis pro Kilogramm Kartoffeln:\n",
      "\\[ \\text{Preis pro Kilogramm} = \\frac{8 \\text{ Euro}}{4 \\text{ Kilogramm}} = 2 \\text{ Euro pro Kilogramm} \\]\n",
      "\n",
      "Nun wissen wir, dass 1 Kilogramm Kartoffeln 2 Euro kostet. Um den Preis für 3 Kilogramm zu berechnen, multiplizieren wir den Preis pro Kilogramm mit der Anzahl der Kilogramm, die wir kaufen möchten:\n",
      "\\[ \\text{Preis für 3 Kilogramm} = 3 \\text{ Kilogramm} \\times 2 \\text{ Euro pro Kilogramm} = 6 \\text{ Euro} \\]\n",
      "\n",
      "Daher kosten 3 Kilogramm Kartoffeln 6 Euro.\n"
     ]
    }
   ],
   "source": [
    "reason_prompt = ChatPromptTemplate.from_template(\"\"\"You are a teacher answering questions of students. Provide thorough reasoning for your answer.\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "reason_chain = reason_prompt | llm | StrOutputParser()\n",
    "\n",
    "answer = reason_chain.invoke({\"input\": \"Ein Sack mit 4 Kilo Kartoffeln kostet 8 Euro. Wie teuer sind 3 Kilogram der Kartoffeln?\"})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answer = reason_chain.invoke({\"input\": \"\"\"\n",
    "In Deutschland verbraucht jeder Mensch im Jahr durchschnittlich 350 kg Holz (Papier, Brennholz, Möbel usw.). Ein Festmeter Holz (ein m³ vollgefüllt) wiegt etwa 700 kg.\n",
    "Ein hundertjähriger Baum hat ca. 2,5 Festmeter. Ein Menschenleben dauert etwa 75 Jahre.\n",
    "\n",
    "    Wie viel wiegt ein solcher Baum?\n",
    "    In welcher Zeit verbraucht man das Holz eines 100-jährigen Baumes?\n",
    "    Wie viel solche Bäume verbrauchen wir im Leben?\n",
    "    Wie viele „Baumjahre\" verbrauchen wir im Leben?\n",
    "\"\"\"})  # Ergebnis: 1.) 1750 kg      2.) 5 Jahren      3.) 15 Bäume       4.) 1500 Baumjahre\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm_reason = AzureChatOpenAI(\n",
    "    api_key=azure_key,\n",
    "    api_version=azure_version,\n",
    "    azure_deployment=azure_reasoning,\n",
    "    model=azure_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n",
    "\n",
    "reason_chain = reason_prompt | llm_reason | StrOutputParser()\n",
    "\n",
    "answer = reason_chain.invoke({\"input\": \"\"\"\n",
    "Leonardo macht sich wieder auf den Weg mit seinem Bugatti Veyron.\n",
    "Er fährt wieder von Pisa nach Mailand mit durchschnittlich 380 km/h.\n",
    "Er braucht dafür eine Dreiviertelstunde. Auf dem Rückweg landet er wiederum in einem Stau. Der Stau ist 10 km lang und er braucht dafür 1,5 Stunden.\n",
    "Danach kommt nochmals 60 km langsam fahrender Verkehr von 40 km/h. Danach kann Leonardo wieder normal weiterfahren.\n",
    "Insgesamt ist seine Durchschnittsgeschwindigkeit, gemessen für den Hin- und Rückweg, 114 km/h.\n",
    "Was ist seine Durchschnittsgeschwindigkeit auf dem Rückweg, auf dem Abschnitt ohne Stau oder langsam fahrenden Verkehr?\"\"\"})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "Now tell me which intent the following question has.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "Now tell me which intent the following question has. Only answer with two/three words.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "\n",
    "Consider the following examples:\n",
    "\n",
    "Question: Let's go to Tokyo next week.\n",
    "Answer: Booking a flight\n",
    "\n",
    "Question: Get some pizza delivered for this evening.\n",
    "Answer: Ordering food\n",
    "\n",
    "Question: How about a movie at 8pm at the cinema?\n",
    "Anmswer: Getting a reservation\n",
    "\n",
    "Now tell me which intent the follolwing question has. Only answer with two/three words.\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.tools.ddg_search import DuckDuckGoSearchRun\n",
    "\n",
    "ddg = DuckDuckGoSearchRun()\n",
    "\n",
    "ddg_tool = Tool.from_function(\n",
    "    func = ddg.run,\n",
    "    name = \"duck_duck_go_search\",\n",
    "    description = \"Search DuckDuckGo for a query about current events.\",\n",
    ")\n",
    "\n",
    "tools = [ddg_tool]\n",
    "\n",
    "ddg_tool.run(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_prompt = SystemMessage(content=\"You are a helpful assistant. You can use the tools provided to help answer questions.\")\n",
    "agent = create_react_agent(\n",
    "    tools=tools, model=llm, prompt=agent_prompt, debug=True,\n",
    ")\n",
    "agent.invoke({\"messages\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_agent_prompt = SystemMessage(content=\"You are a great AI-Assistant and only answer in 10 words. You can use the tools provided to help answer questions.\")\n",
    "agent = create_react_agent(\n",
    "    tools=tools, model=llm, prompt=new_agent_prompt, debug=True\n",
    ")\n",
    "agent.invoke({\"messages\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})\n",
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "simple_summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand. Keep it funky!\n",
    "\n",
    "Text: {input}\"\"\")\n",
    "simple_summary_chain = {\"input\": RunnablePassthrough()} | simple_summary_prompt | llm\n",
    "\n",
    "summary_tool = Tool(\n",
    "    name=\"funky_summary_tool\",\n",
    "    func=simple_summary_chain.invoke,\n",
    "    description=\"Use this tool to do a funky summary. Make sure you get the text to do a summary of first.\"\n",
    ")\n",
    "tools.append(summary_tool)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    tools=tools, model=llm, prompt=agent_prompt, debug=True,\n",
    ")\n",
    "result = agent.invoke({\"messages\": \"Funky summarize the christmas speech 2023 of Frank-Walter Steinmeier?\"})\n",
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "retrieval_agent_prompt = SystemMessage(content=\"\"\"\n",
    "You are a great AI-Assistant to answer question based provided documents. \n",
    "You are given access to an retrieval too. Use it to get information you need - do not answer based on general knowledge!\n",
    "If you do not get the necessary information from the retrieval tool, try to adjust your question.\n",
    "Always try to search for single facts and not for complete answers!\n",
    "Only answer based on sources provided and cite the source after each fact!\n",
    "\"\"\")\n",
    "\n",
    "tool_retrieval_chain = {\"input\": RunnablePassthrough()} | retrieval_chain\n",
    "\n",
    "retrieval_tool = Tool(\n",
    "    name=\"retrieval_tool\",\n",
    "    func=tool_retrieval_chain.invoke,\n",
    "    description=\"Use this tool to retrieve information from a set of documents. Only search for one word facts!\",\n",
    ")\n",
    "\n",
    "retrieval_agent = create_react_agent(\n",
    "    tools=[retrieval_tool], model=llm, prompt=retrieval_agent_prompt,\n",
    ")\n",
    "\n",
    "result = retrieval_agent.invoke({\"messages\": \"What is the common ground of pub quizzes and llms?\"})\n",
    "print('\\n'.join([f\"{message.__class__.__name__}: {str(message)}\" for message in result[\"messages\"]]))\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "memory = MemorySaver()\n",
    "memory_agent = create_react_agent(llm, prompt=retrieval_agent_prompt, tools=[retrieval_tool], checkpointer=memory)\n",
    "\n",
    "result = memory_agent.invoke({\"messages\": \"What is the common ground of pub quizzes and llms?\"}, config)\n",
    "print('\\n'.join([f\"{message.__class__.__name__}: {str(message)}\" for message in result[\"messages\"]]))\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "result = memory_agent.invoke({\"messages\": \"Translate that to german.\"}, config)\n",
    "print('\\n'.join([f\"{message.__class__.__name__}: {str(message)}\" for message in result[\"messages\"]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from mimetypes import guess_type\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Function to encode a local image into data URL \n",
    "def local_image_to_data_url(image_path):\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    # Default to png\n",
    "    if mime_type is None:\n",
    "        mime_type = 'image/png'\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "\n",
    "prompt_template =  HumanMessagePromptTemplate.from_template(\n",
    "            template=[\n",
    "                {\"type\": \"text\", \"text\": \"Summarize this image\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": \"{encoded_image_url}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "summarize_image_prompt = ChatPromptTemplate.from_messages([prompt_template])\n",
    "\n",
    "gpt4_image_chain = summarize_image_prompt | llm \n",
    "\n",
    "img_file = \"data/images/userguide.jpg\"\n",
    "page3_encoded = local_image_to_data_url(img_file)\n",
    "\n",
    "answer = gpt4_image_chain.invoke(input={\"encoded_image_url\":page3_encoded})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "img_file = \"data/images/teambudget.jpg\"\n",
    "page3_encoded = local_image_to_data_url(img_file)\n",
    "\n",
    "answer = gpt4_image_chain.invoke(input={\"encoded_image_url\":page3_encoded})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=azure_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=azure_whisper,\n",
    "    api_version=azure_version,\n",
    ")\n",
    "\n",
    "def get_transcript(audio_file):\n",
    "    if not os.path.exists(audio_file):\n",
    "        audio_file = \"./data/audio/\" + audio_file\n",
    "    client.audio.with_raw_response\n",
    "    return client.audio.transcriptions.create(\n",
    "        file=open(audio_file, \"rb\"),            \n",
    "        model=\"whisper\",\n",
    "        language=\"de\",\n",
    "    ).text\n",
    "\n",
    "audio_test_file = \"./data/audio/newyear2023.mp3\"\n",
    "print(get_transcript(audio_test_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# audio\n",
    "def get_audio_doc(input):\n",
    "    return {\n",
    "        \"context\": [Document(\n",
    "            page_content=get_transcript(input),\n",
    "            metadata={\n",
    "                \"source\": \"audio\"\n",
    "            }\n",
    "        )]\n",
    "    }\n",
    "audio_summary_chain = get_audio_doc | summary_chain\n",
    "#answer = audio_summary_chain.invoke(\"./data/audio/newyear2023.mp3\")\n",
    "#print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def available_audio_files(input):\n",
    "    return \"./data/audio/newyear2023.mp3 - new years eve speech 2023 of Olaf Scholz\\n\"\\\n",
    "        \"./data/audio/newyear2016.mp3 - new years eve speech 2016 of Angela Merkel\\n\" \\\n",
    "        \"./data/audio/christmas2019.mp3 - christmas speech 2019 of Frank-Walter Steinmeyer\\n\"\n",
    "\n",
    "audio_file_tool = Tool(\n",
    "    name=\"audio_file_tool\",\n",
    "    func=available_audio_files,\n",
    "    description=\"Use this tool to see which new years eve speeches are available as an audio file.\"\n",
    ")\n",
    "\n",
    "audio_tool = Tool(\n",
    "    name=\"audio_transcript_tool\",\n",
    "    func=get_audio_doc,\n",
    "    description=\"Use this tool to get the transcript of the speech in a given audio file.\"\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    tools=[audio_file_tool, audio_tool, summary_tool],\n",
    "    model=llm,\n",
    "    prompt=agent_prompt,\n",
    "    debug=True,\n",
    ")\n",
    "agent.invoke({\"messages\": \"Summarize the new years eve speech 2023 of Olaf Scholz using the audio and summary tool?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "SQL_AGENT_PROMPT = \"\"\"You are an agent designed to interact with a SQL database, to show database outputs as tables and to generate plots.\n",
    "Given an input question, create a syntactically correct SQL query to run, then look at the results of the query and return the answer.\n",
    "If no specific number of results is requested, return the top 5 results. If a specific range (e.g. dates) are requested, don't limit the number of results.\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "Query for the relevant columns given e question.\n",
    "You have access to tools for interacting with the database.\n",
    "Only use the below tools. Only use the information returned by the below tools to construct your final answer.\n",
    "You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\n",
    "Always give the results received from the database. They must be displayed with a custom markdown element (table).\n",
    "This is an example of how to use it:\n",
    "\n",
    "```db_result\n",
    "| Country | TotalCost |\n",
    "|---------|-----------|\n",
    "| Germany | 101       |\n",
    "| Italy   | 104       |\n",
    "| France  | 98        |\n",
    "```\n",
    "\n",
    "Use **bold** markdown syntax to highlight the most relevant part of your answer (e.g. a calculation result).\n",
    "\n",
    "Before making any query, check the available table names and correct schema of the table to avoid errors.\n",
    "\n",
    "Hint: to get a specfic part of a datetime, use the strftime(format, date) function with %Y for years, %m for months and %d for days.\n",
    "\n",
    "Remember: ALWAYS output the returned data! ALWAYS use the custom markdown syntax for this (triple backtick and db_result)!\n",
    "ALWAYS query the database if any data is requested, even if the user does not refer to the database. Assume that any request refers to DB data and execute the corresponding function.\n",
    "The database is the single source of truth for your answers. If the data cannot be found in the database, state this truthfully.\n",
    "y\n",
    "Double check if you actually used a function to query the DB before including data in your response.\n",
    "\n",
    "If you need to do any calculations, ALWAYS use the SQL database for this. NEVER use your own knowledge to provide the answer to a mathematical calculation.\n",
    "For example, to calculate the sum of 5 and 3, run this query: `SELECT 5 + 3 AS result;`\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_react_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m toolkit \u001b[38;5;241m=\u001b[39m SQLDatabaseToolkit(db\u001b[38;5;241m=\u001b[39mdb, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[1;32m      6\u001b[0m tools \u001b[38;5;241m=\u001b[39m toolkit\u001b[38;5;241m.\u001b[39mget_tools()\n\u001b[0;32m----> 8\u001b[0m sql_agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_react_agent\u001b[49m(\n\u001b[1;32m      9\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     11\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mSQL_AGENT_PROMPT,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m result \u001b[38;5;241m=\u001b[39m sql_agent\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the top 5 tracks by length?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_react_agent' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///chinook.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "sql_agent = create_react_agent(\n",
    "    tools=tools,\n",
    "    model=llm,\n",
    "    prompt=SQL_AGENT_PROMPT,\n",
    ")\n",
    "\n",
    "result = sql_agent.invoke({\"messages\": \"What are the top 5 tracks by length?\"})\n",
    "print('\\n'.join([f\"{message.__class__.__name__}: {str(message)}\" for message in result[\"messages\"]]))\n",
    "#print('\\n'.join([message.content for message in result[\"messages\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "db.run(\"SELECT * FROM track LIMIT 5;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class WikiJson(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "    summary: str\n",
    "\n",
    "llm_structured = AzureChatOpenAI(\n",
    "    api_key=azure_key,\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    model=azure_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ").with_structured_output(WikiJson)\n",
    "\n",
    "answer = llm_structured.invoke(\"What do you know about Pub Quizzes?\", seed=1000)\n",
    "print(answer.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
